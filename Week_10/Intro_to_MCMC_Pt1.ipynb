{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Introduction to MCMC and emcee\n",
    "\n",
    "In this notebook we will go over what MCMC means and what exactly is it that is going on when we run an MCMC algorithm such as Metropolist-Hasting or a code like emcee. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is MCMC?\n",
    "\n",
    "Markov Chain Monte Carlo (MCMC) is a class of algorithms used to sample from a probability distribution. It is particularly useful when dealing with high-dimensional spaces where traditional sampling methods are inefficient. MCMC methods construct a Markov chain that has the desired distribution as its equilibrium distribution. By simulating the Markov chain, we can obtain samples from the target distribution. As astronomers we do a lot of work trying to estimate physical paramters on astrophysical sources such as stars, galaxies and cosmology. Most of the time the target distribution we are after is getting a handle on the parameters for a certain model and the uncertainties on those parameters. MCMC comes in handy to sample the parameter spaces for these complex models and allows us to see how constrained certain parameters are. \n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Markov Chain**: A sequence of random variables where the distribution of each variable depends only on the state of the previous one.\n",
    "- **Monte Carlo**: Refers to the use of random sampling to estimate numerical results.\n",
    "- **Equilibrium Distribution**: The distribution to which the Markov chain converges after a large number of steps. In the Bayesian context this equilibrium distribution is the *Posterior* Distribution which we will cover later in this notebook.\n",
    "\n",
    "### Common MCMC Algorithms\n",
    "\n",
    "- **Metropolis-Hastings**: A widely used MCMC algorithm that generates a sequence of samples by proposing moves to new states and accepting or rejecting them based on a certain probability.\n",
    "- **Gibbs Sampling**: A special case of the Metropolis-Hastings algorithm where each variable is sampled from its conditional distribution given the current values of the other variables.\n",
    "\n",
    "### Applications\n",
    "\n",
    "MCMC methods are used in various fields such as:\n",
    "\n",
    "- Bayesian statistics for posterior distribution sampling\n",
    "- Computational physics for simulating systems with many degrees of freedom\n",
    "- Machine learning for training models with complex likelihood functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we use MCMC to begin with? \n",
    "\n",
    "# Introducing Bayesian Statistics\n",
    "\n",
    "The reason why we go ahead and use MCMC to begin with is because we are using a Bayesian framework to understand the features we are studying. To start our Bayesian journey we need to go over the main equation that is driving Bayesian Statistics forward, Bayes Theorem:\n",
    "\n",
    "$P(\\theta | x) = \\frac{P(x|\\theta)P(\\theta)}{P(x)}$\n",
    "\n",
    "Let us unpack the above equation:\n",
    "\n",
    "$P(\\theta|x)$ is the probability of acquiring certain parameters $\\theta$ provided the data, $x$ and the quantity we are after. In Bayesian terms this is the *Posterior* Distribution. $P(x|\\theta)P(\\theta)$ is a combination of the *likelihood* function $P(x|\\theta)$, which is the probability of acquiring the data for a set of input quantities $\\theta$ and $P(\\theta)$ is the *prior* which is the probability of getting those parameters.\n",
    "\n",
    "The denominator is the total probability of acquiring the data, a way to get this is by integrating over all possible values of $\\theta$:\n",
    "\n",
    "$P(x) = \\int P(x, \\theta) d\\theta$\n",
    "\n",
    "However, this integral is non-trivial to solve most of the time. This is reason why we use MCMC, as a way to get back the posterior distribution we are interested in. This is achieved because the Posterior distribution can be *approximated* by the product of the likelihood and prior. \n",
    "\n",
    "This can be seen in Bayes Theorem where if we ignore the denominator the Posterior distribution goes as: \n",
    "\n",
    "$P(\\theta | x) \\approx P(x|\\theta)p(\\theta)$\n",
    "\n",
    "\n",
    "We will exploit this feature of approximating the posterior by the product of Likelihood and Prior below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does MCMC work?\n",
    " \n",
    "The first thing we have to do when starting MCMC is to provide an MCMC algorithm with an initial guess on the parameters we are interested in. Then you propose a move jump from the starting point to another point, these jumps can be as simple or complex as you want. Let us say for the sake of this example that you propose a jump that is a normal distribution centered at 0 and has a standard deviation of 1. So let us take our starting value as 1, the jumps we can take can be anything within the x-values as shown in the plot below:\n",
    "\n",
    "![alt text](jump_norm.png \"jumps\")\n",
    "\n",
    "We can see the normal distribution above and the x-values are the potential jumps that we can take and the probability of jumping to those x-values are shown on the y-axis. This means that a jump value of 0 is more likely than any other jumps. \n",
    "\n",
    "So let us say that we drew a jump value of -0.5 that means that we will take the current value update it with a value of -0.5 by adding it to our current value and that will give us another value, -0.5 in this case. We then see if this new value that we are at is better than the previous value we were at. \n",
    "\n",
    "So how do we quantify this as a better value?\n",
    "\n",
    "The way we do this is by comparing the likelihood of getting that parameter against the priors on that parameters and we see which parameter has a higher value. We then take the ratio of the two likelihoods and then we will accept the new value if this ratio is higher than a random value drawn between 0 and 1. If it is then we accept the jump, if it is not then we stay at the current value and do another jump to a different value. Then we repeat this process until the parameters converge. \n",
    "\n",
    "These values that jump from one value to another have a term for them called \"walkers\". Their role is to explore or walk around the parameter space to explore the values that have the highest likelihood.\n",
    "\n",
    "Things to note about the jumps to look out for:\n",
    "\n",
    "- The jump size is something that can take some fine tuning because you want the jumps to be distinct from the current value but you do not want to take too big of a jump that you miss out on values. \n",
    "\n",
    "\n",
    "In Short: \n",
    "\n",
    "1. Provide an intitial guess on the parameters\n",
    "2. propose a jump condition from the initial guess\n",
    "3. compare the new value to the old value by comparing their likelihoods\n",
    "4. Apply randomness by comparing the ratio of the likelihoods to a random number between 0-1\n",
    "    a. If likelihod ratio is greater than this random number we accept the new value\n",
    "    b. If it is not we stay at the current value\n",
    "5. Repeat steps 2-4 for some time until the walkers have converged\n",
    "6. You can now analyze the distribution of values \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example with emcee: Fitting a Line\n",
    "\n",
    "We will go over implimenting MCMC using a package called emcee in the cells below. We will go over some of the basic frameowrk of MCMC using a simple example of fitting a line to some data and then in Part 3 we will expand upon this by fitting a Gaussian model to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Generating the data that we are trying to fit\n",
    "\n",
    "The data we are going to try to fit is a set of data that has a linear relation. That is a function that has the form:\n",
    "\n",
    "y = mx + b\n",
    "\n",
    "The parameters that we are going to try to fit are the slope (m) and y-intercept (b). Let us generate the data and add some noise to them so the data do not lie exactly on a line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x116e34440>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(134)\n",
    "\n",
    "# Choose the \"true\" parameters.\n",
    "m_true = -0.9594\n",
    "b_true = 4.294\n",
    "f_true = 0.534\n",
    "\n",
    "# Generate some synthetic data from the model.\n",
    "N = 50\n",
    "x = np.sort(10 * np.random.rand(N))\n",
    "yerr = 0.1 + 0.5 * np.random.rand(N)\n",
    "y = m_true * x + b_true\n",
    "y += np.abs(f_true * y) * np.random.randn(N)\n",
    "y += yerr * np.random.randn(N)\n",
    "\n",
    "plt.errorbar(x, y, yerr=yerr, fmt=\".k\", capsize=0, label = 'Data')\n",
    "x0 = np.linspace(0, 10, 500)\n",
    "plt.plot(x0, m_true * x0 + b_true, \"k\", alpha=0.3, lw=3, label = 'True Model')\n",
    "plt.xlim(0, 10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Generate Likelihood Function\n",
    "\n",
    "The first step we need to do when using emcee is start with a likelihood function. This function encapsulates how likely a model can match to the data. One of the ways that we do this is by using a version of the $\\chi^2$ goodness of fit criteria as our likelihood function. We want a function that can find the smallest difference between the data and the model, which a $\\chi^2$ can do as the functional form for that is:\n",
    "\n",
    "$\\chi^2 = \\sum_1^N \\frac{(y - f(\\theta))^2}{\\sigma^2}$\n",
    "\n",
    "In the above equation N is the number of data points you are fitting, y is the y-data, f($\\theta$) is the model for a given $\\theta$ value and $\\sigma$ is the error on the y-data. \n",
    "\n",
    "For this example, and all real world example, the model will almost never cross through all the data even with the inclusion of the errors so one thing we will add is a fudge factor to take this into account. This fudge factor will be another parameter that we are fitting and will show up in our likelihood calculations as shown below.\n",
    "\n",
    "For our likelihood function below we are going to assume gaussian errors and as a result this will add another term into the likelihood function. Our final likelihood is actually going to be:\n",
    "\n",
    "$log(L) = 0.5  \\sum_1^N \\frac{(y - f(\\theta))^2}{\\sigma^2} + ln(\\sigma_2)$\n",
    "\n",
    "Where:\n",
    "\n",
    "$\\sigma_2 = \\sigma + f(\\theta)* e^{2ln(f)}$\n",
    "\n",
    "So in a sense we are scaling the errors by an additional factor determined by the fudge factor $ln(f)$. $\\sigma$ is the y-error and $f(\\theta)$ is the model for the given set of parameters $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(theta, x, y, yerr):\n",
    "    m, b, log_f = theta\n",
    "    model = m * x + b\n",
    "    sigma2 = yerr**2 + model**2 * np.exp(2 * log_f)\n",
    "    return -0.5 * np.sum((y - model) ** 2 / sigma2 + np.log(sigma2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Making the Prior Function\n",
    "\n",
    "Now that we have made the Likelihood function, we can move ahead and make our prior function. This prior will restrict the parameter bounds if we need them to. This function will ensure that the parameters explores within MCMC and emcee stay within the bounds determined by the prior. \n",
    "\n",
    "## Why do we need priors?\n",
    "\n",
    "Sometimes priors are needed to help MCMC codes and algorithms to efficiently explore the parameter spaces we are interested. By default, if no priors are provided, the walkers can explore every possible value in the parameter space even if the value for that parameter may be unphysical or not possible. \n",
    "\n",
    "For example, if you have a model and one of the parameters is mass, we know that mass is always positive. So one easy prior to impose is to ensure that mass always stays positive, providing a lower limit to mass you have eliminated half of the parameter space. \n",
    "\n",
    "You can also use astrophysical knowledge to construct what an upper limit would be. If you are working with a code to estimate galaxy masses you can use your knowledge of galaxies to construct an upper limit of the stellar mass based off of cosmology and some star formation efficiency. \n",
    "\n",
    "### Note: \n",
    "Not all parameters need priors, but if you have some prior knowledge of what values a parameter should be around it would be wise to incorporate that into the prior function for more efficient parameter space exploration of the walker. \n",
    "\n",
    "# Priors for this example:\n",
    "\n",
    "Since we are fitting a line that includes the parameters: m, b and $ln(f)$, we will need to be strategic about what priors to impose. \n",
    "\n",
    "For the slope, we cannot apply a strictly positive prior as the slope can be either negative or positive but we can impose a lower and upper bound. For the y-intercept we can look at the data and see that the y-intercept has to be above 0 but and below 10. For $ln(f)$, since f is a fraction $ln(f)$ can be negative or positive for this example we will restrict it to -10 and 1.\n",
    "\n",
    "For this example we are going to use the following bounds for each of the parameters. \n",
    "\n",
    "$-5 < m < 0.5$, \n",
    "$0 < b < 10$, \n",
    "$-10 < ln(f) < 1$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(theta):\n",
    "    m, b, log_f = theta\n",
    "    if -5.0 < m < 0.5 and 0.0 < b < 10.0 and -10.0 < log_f < 1.0:\n",
    "        return 0.0\n",
    "    return -np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the prior function\n",
    "\n",
    "Now that we have made the prior functions let us provide it with some theta values and see what the output of the prior returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Prior value for theta1 is: -inf\n",
      "The Prior value for theta2 is: 0.0\n",
      "The Prior value for theta3 is: -inf\n"
     ]
    }
   ],
   "source": [
    "#Working with the prior functions\n",
    "\n",
    "#making some random theta values and seeing what we get from the prior\n",
    "theta1 = (-10, 5, .1)   \n",
    "theta2 = (-0.5, 9, -1)\n",
    "theta3 = (0.1, 10, 1)\n",
    "\n",
    "prior_theta1 = log_prior(theta1)\n",
    "prior_theta2 = log_prior(theta2)\n",
    "prior_theta3 = log_prior(theta3)\n",
    "\n",
    "print(f'The Prior value for theta1 is: {prior_theta1}')\n",
    "print(f'The Prior value for theta2 is: {prior_theta2}')\n",
    "print(f'The Prior value for theta3 is: {prior_theta3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looks like we got 1 value of 0 and two with -inf, let us look into why that is:\n",
    "\n",
    "#theta 1 is (-10, 5, .1), which is m, b and lnf respectively\n",
    "#if we look at the prior bounds on each of the parameters, \n",
    "#we see that m is between -5 and 0.5, b is between 0 and 10, and lnf is between -10 and 1\n",
    "#since -10 for m is outside the bounds, the prior value is -inf\n",
    "\n",
    "#theta 2 is (-0.5, 9, -1), which is m, b and lnf respectively\n",
    "#if we look at the prior bounds on each of the parameters,\n",
    "#we see that m is between -5 and 0.5, b is between 0 and 10, and lnf is between -10 and 1\n",
    "#None of the values for theta are outside of this bound and so we get a vaue of 0\n",
    "\n",
    "#theta 3 is (0.1, 10, 1), which is m, b and lnf respectively\n",
    "#if we look at the prior bounds on each of the parameters,\n",
    "#we see that m is between -5 and 0.5, b is between 0 and 10, and lnf is between -10 and 1\n",
    "#we see that 10 for b is just outside the bounds as it needs to be strictly less than 10 and that \n",
    "#is why we get a value of -inf\n",
    "\n",
    "\n",
    "#provide some theta values to test the prior functions and make an example:\n",
    "# 1. where the prior is 0\n",
    "# 2. the prior returns -inf\n",
    "theta4 = \n",
    "theta5 = \n",
    "\n",
    "print(f'The Prior value for theta4 is: {prior_theta4}')\n",
    "print(f'The Prior value for theta5 is: {prior_theta5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Merging the Likelihood and Prior into a single function\n",
    "\n",
    "Now that we have made the prior and likelihood function we need to combine them together, similar to the numerator in Bayes Theorem. One fact that we will exploit is that if we take the log of the posterior distributions we can convert the multiplication of the likelihood and prior functions into a sum. Where:\n",
    "\n",
    "$Ln(Posterior) = Ln(Likelihood) + Ln(Prior)$\n",
    "\n",
    "Let us make that function below and turn that into a function titled log_probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_probability(theta, x, y, yerr):\n",
    "    lp = log_prior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(theta, x, y, yerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Generating Walkers\n",
    "\n",
    "Now that we have the log_probability function we are one step closer to using emcee, the last thing we need to do is to provide emcee with some initial guesses on our parameters. \n",
    "\n",
    "With computers being as powerful as they are we can initialize emcee with not just one walkers but any arbitrary number of walkers, each with its own initial starting point. The way we will do this is by starting off with an intial guess array and perturbing that slightly and drawing 32 random numbers from a distribution to alter their starting position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "# Initial guess   m,   b, log(f)        #generating 32 random walkers for each of the parameters\n",
    "pos =  np.array([-0.8, 4, 0.1])+ 1e-4 * np.random.randn(32, 3)\n",
    "\n",
    "nwalkers, ndim = pos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Running Emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = emcee.EnsembleSampler(nwalkers,         #need to specify the number of walkers\n",
    "                                ndim,             #need to specify the number of dimensions\n",
    "                                log_probability,  #need to specify the log-probability function\n",
    "                                args=(x, y, yerr) #any additional arguments needed into log-probability function goes here\n",
    "                                )\n",
    "\n",
    "sampler.run_mcmc(pos, 5000, progress=True) #This runs emceee for 5000 steps, meaning that the walkers move 5000 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Post-Emcee Run Analysis\n",
    "\n",
    "Once emcee has finished running, we need to make sure that emcee has converged. The way to do this is by plotting \"trace\" plots which are all the values each of the walkers have explored. By the end of the run they should be moving around the final values. The walkers for your parameters should look like this towards the end of the emcee run.\n",
    "\n",
    "![alt text](converge_emcee.png \"converge_emcee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the traces of the walkers\n",
    "fig, axes = plt.subplots(3, figsize=(10, 7), sharex=True)\n",
    "samples = sampler.get_chain()\n",
    "labels = [\"m\", \"b\", \"log(f)\"]\n",
    "for i in range(ndim):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples))\n",
    "    ax.set_ylabel(labels[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "axes[-1].set_xlabel(\"step number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "flat_samples = sampler.get_chain(discard=100, thin=15, flat=True)\n",
    "fig = corner.corner(\n",
    "    flat_samples, \n",
    "    labels=labels, \n",
    "    truths=[m_true, b_true, np.log(f_true)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_dictionary = {'m': flat_samples[:, 0], \n",
    "                   'b': flat_samples[:, 1], \n",
    "                   'logf': flat_samples[:, 2]}\n",
    "\n",
    "df = pd.DataFrame(data_dictionary)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('emcee_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Quantifying Parameters and Uncertainties\n",
    "\n",
    "Once we have checked that emcee has converged and the output looks good. We now have a distribution for each parameter so how do we quantify this for a paper or poster. One of the ways we can do this is by quoting the percentiles of this distribution. Specifically, the 50th percentile or median would be your value you quote and the resulting uncertainty is the 16th and 84th percentile. To get a proper lower and upper error you would need to do the following; lower_err = median - 16th percentile, upper_err = 84th percentile - median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l16, med, u84 = np.percentile(flat_samples, q = (16, 50, 84), axis=0)\n",
    "lerr = med - l16\n",
    "uerr = u84 - med"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
