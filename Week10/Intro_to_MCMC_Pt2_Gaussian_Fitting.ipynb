{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef654cb",
   "metadata": {},
   "source": [
    "# Spectra Fitting with Emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c39c76c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "import emcee\n",
    "from scipy.interpolate import Akima1DInterpolator\n",
    "from scipy.optimize import curve_fit\n",
    "import seaborn as sb\n",
    "\n",
    "plt.rcParams['axes.linewidth'] = 1.5\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.size\"] = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61344f5",
   "metadata": {},
   "source": [
    "In this notebook we will expand upon the line fitting tools you have acquired in part 2 and expand upon them by using a more complex model. Emcee is a python package that does parameter estimation in a Bayesian way. The advantage to this approach is that you will be able to not only get the best fit values on your parameters but also the corresponding uncertainties. To make emcee work properly you will be responsible with: \n",
    "\n",
    "1. Cleaning the data\n",
    "2. Generating the Model\n",
    "3. Computing the likelihood functions\n",
    "4. Coming up with any priors for your parameters\n",
    "4. Executing emcee\n",
    "\n",
    "But fear not we will go over these steps in the following cells in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc8bff8",
   "metadata": {},
   "source": [
    "# 1. Cleaning the data\n",
    "\n",
    "One of the first things we have to do is to ensure that the data we are working with is in a good condition to perform scientific analysis. While a lot of work has been done to ensure that the data you are working with is as good as possible there can be edge cases or something along the data reduction pipeline did not work as intended. \n",
    "\n",
    "One of the things you can do is to do some descriptive analysis such as finding the minimum, maximum, mean, median, standard deviation, and plotting the data with simple plots or histograms to get a sense of how the data is distributed. So let us do this with the following file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c0913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in the file\n",
    "\n",
    "# Open the FITS file\n",
    "file = fits.open(\"hlsp_ceers_jwst_nirspec_nirspec4-004315_comb-mgrat_v0.7_x1d-masked.fits\")\n",
    "\n",
    "# Access the data within the first HDU (assuming the table is in the first HDU)\n",
    "data = file[1].data\n",
    "\n",
    "# Get the column names\n",
    "column_names = data.columns.names\n",
    "\n",
    "# Print the column names\n",
    "for name in column_names:\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77d49e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the flux, flux error and wavelength\n",
    "flux = data['FLUX']\n",
    "flux_error = data['FLUX_ERROR']\n",
    "wavelength = data['WAVELENGTH']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b8e61b",
   "metadata": {},
   "source": [
    "Now let us check out the data and see what we are working with with some descriptive statistics and basic plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c5e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_flux = np.amax(flux)\n",
    "min_flux = np.amax(flux)\n",
    "\n",
    "min_ferr = np.amin(flux_error) \n",
    "max_ferr = np.amax(flux_error) \n",
    "\n",
    "min_wave = np.amin(wavelength)\n",
    "max_wave = np.amax(wavelength)\n",
    "\n",
    "print(f'Minimum flux: {min_flux}')\n",
    "print(f'Maximum flux: {max_flux}')\n",
    "print(f'Minimum flux error: {min_ferr}')\n",
    "print(f'Maximum flux error: {max_ferr}')\n",
    "print(f'Minimum wavelength: {min_wave}')\n",
    "print(f'Maximum wavelength: {max_wave}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82be7abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting up the data\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.step(wavelength, flux, color='black', where = 'mid', label='Flux')\n",
    "plt.errorbar(wavelength, flux, yerr=flux_error, fmt='o', color='black', label='Flux Error')\n",
    "plt.xlabel('Wavelength (microns)')\n",
    "plt.ylabel('Flux (Jy)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d612b3",
   "metadata": {},
   "source": [
    "Hmm, looks like we have some NaN values and the spectra looks odd with some large errors. In order for us to do analysis we need a way to get rid of these pesky NaN values that could be causing the large spikes that we see in the spectrum above. Luckily for us numpy has some nifty functions to detect NaNs and we can use these to mask them out. Let us do that below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f34311e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we had some nan values so we get a nan mask and apply the mask to the data\n",
    "#np.isnan is a function that returns True if the value is nan and False if it is not\n",
    "#~ is a not operator so it will return True if the value is not nan and False if it is nan\n",
    "flux_nan = ~np.isnan(flux)\n",
    "flux_error_nan = ~np.isnan(flux_error)\n",
    "\n",
    "#combining the two masks\n",
    "master_mask = flux_nan & flux_error_nan\n",
    "\n",
    "#applied mask to the data\n",
    "wavelength_cleaned = wavelength[master_mask]\n",
    "flux_cleaned = flux[master_mask]\n",
    "flux_error_cleaned = flux_error[master_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39351400",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_flux = np.amax(flux_cleaned)\n",
    "min_flux = np.amax(flux_cleaned)\n",
    "\n",
    "min_ferr = np.amin(flux_error_cleaned) \n",
    "max_ferr = np.amax(flux_error_cleaned) \n",
    "\n",
    "min_wave = np.amin(wavelength_cleaned)\n",
    "max_wave = np.amax(wavelength_cleaned)\n",
    "\n",
    "print(f'Minimum flux: {min_flux}')\n",
    "print(f'Maximum flux: {max_flux}')\n",
    "print(f'Minimum flux error: {min_ferr}')\n",
    "print(f'Maximum flux error: {max_ferr}')\n",
    "print(f'Minimum wavelength: {min_wave}')\n",
    "print(f'Maximum wavelength: {max_wave}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325f4267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the Spectrum\n",
    "plt.figure(figsize = (12, 7))\n",
    "\n",
    "plt.plot(wavelength_cleaned, flux_cleaned, color = 'black', alpha = 0.5)\n",
    "\n",
    "plt.xlabel(r'Wavelength [$\\mu$m]')\n",
    "plt.ylabel('Flux')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86287e91",
   "metadata": {},
   "source": [
    "Look at that!! The spectrum looks really nice and by removing the NaNs we are now able to see it and get some descriptive statistics from the spectrum! Now that we have the spectrum we can go ahead to step 2 of Emcee fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a4de4",
   "metadata": {},
   "source": [
    "# 2 Model Generation\n",
    "\n",
    "When fitting a single line the model you will be using is going to be a combination of a Gaussian and a line. The gaussian will fit the shape of the emission line and the line will fit any continuum offset near the emission line. So let us define our models separatey and then merge them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa84c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, A, mu, sigma):\n",
    "    \n",
    "    '''\n",
    "    Gaussian Model for Line Fitting. This is of the form:\n",
    "    \n",
    "    Gaussian = Ae^(-(x-mu)^2/(2sigma^2))\n",
    "    \n",
    "    Input\n",
    "    -------------\n",
    "    x: array or single value to evaluate the Gaussian \n",
    "    A: amplitude of the Gaussian\n",
    "    mu: Center of the Gaussian\n",
    "    sigma: the standard deviation of the Gaussian\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    --------------\n",
    "    Evaluated Gaussian for the given A, mu and sigma at the point(s) in x\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return A * np.exp(-(x - mu)**2/ (2*sigma**2))\n",
    "\n",
    "def line(x, m, b):\n",
    "    \n",
    "    '''\n",
    "    Continuum of the spectra using y = b\n",
    "    \n",
    "    Input\n",
    "    ------------\n",
    "    x: array of values\n",
    "    b: value to plot y = b\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    ------------\n",
    "    An array of values at b, the same size as x\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return  m*x + b\n",
    "\n",
    "def line_model(x, A, mu, sigma, m, b):\n",
    "    \n",
    "    '''\n",
    "    Emission Line model using Gaussian and the continuum\n",
    "    \n",
    "    Inputs\n",
    "    ------------\n",
    "\n",
    "    x: array of values to evaluate the Gaussian \n",
    "    A: amplitude of the Gaussian\n",
    "    mu: Center of the Gaussian\n",
    "    sigma: the standard deviation of the Gaussian\n",
    "    b: value to plot y = b\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    return gaussian(x, A, mu, sigma) + line(x, m, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3851cd",
   "metadata": {},
   "source": [
    "# 3 Making the Likelihood Function\n",
    "\n",
    "For the likelihood function most of the time what we are trying to improve is how close the model can reproduce our data. This means that we are trying to minimize the difference between the data and model for a certain set of parameters. The likelihood function encapsulates this and we have provided you with a likelihood function below that does this. Please study the inputs and understand what each line of code is doing as a good understanding of this function can help with debugging and interpret results later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa189832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(theta, x, y, yerr):\n",
    "    '''\n",
    "    This is the likelihood function we are using for emcee to run\n",
    "    \n",
    "    This likelihood function is the maximum likelihood assuming gaussian errors.\n",
    "    \n",
    "    '''\n",
    "    ################\n",
    "\n",
    "    #In the context of emcee theta is a tuple that has all the parameters we are trying to fit\n",
    "    #In this case theta = (A, mu, sigma, m, b)\n",
    "    \n",
    "    #in python there is a quick way to pass in a tuple of variables into a function using the * operator\n",
    "    #so if theta = (A, mu, sigma, m, b) then *theta is the same as passing in A, mu, sigma, m, b\n",
    "    #into the function line_model without having to write line_model(x, A, mu, sigma, m, b)\n",
    "\n",
    "    #Making the model of the emission line\n",
    "    model = line_model(x, *theta)\n",
    "    \n",
    "    #To compute the log likelihood we use the formula:\n",
    "    # summation of (y - model)^2/sigma^2, where y is the data, model is the model for the current paramters theta \n",
    "    # and sigma is the error in the data\n",
    "\n",
    "    #getting the log likelihood, this is similar to chi2 = sum((data - model)^2/sigma^2)\n",
    "    lnL = -0.5 * np.sum((y - model) ** 2 / yerr**2)\n",
    "    \n",
    "    return lnL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d994e0fc",
   "metadata": {},
   "source": [
    "# 4. Generating Priors on our Parameters\n",
    "\n",
    "Now that we have a way to let emcee know if the current set of parameters are a good fit to the data, by minimizing the difference between data and model, we can now go ahead and work on our priors. Priors are just bounds that we can impose on our parameters. Having a prior is completely optional but it can help with removing unwanted characteristics in the fitting or having it explore values you know cannot be true. For example, we are fitting emission lines which means that the amplitude for the gaussians will never be negative so you can have a prior to force the amplitude of the gaussian to always be positive. If you have some other prior knowledge on some other parameters then the log_prior function is where you would put those bounds in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce06b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(theta, wave_center, Amp_max):\n",
    "    '''\n",
    "    The prior function to be used against the parameters to impose certain criteria for the fitting making \n",
    "    sure that they do not go and explore weird values\n",
    "    \n",
    "    '''\n",
    "    #Theta values that goes into our emission line Model\n",
    "    A, mu, sigma, b = theta\n",
    "    \n",
    "    #In this example we have some prior knowledge about the wavelength we are fitting and how much they can vary\n",
    "    #so this is why in this prior we are only letting the central wavelength mu vary between a certain range\n",
    "\n",
    "    wave_window_micron = .002\n",
    "\n",
    "    #the left most bound and right most bound that the central wavelength can vary\n",
    "    left_mu = wave_center - wave_window_micron  # this is how much mu can vary\n",
    "    right_mu = wave_center + wave_window_micron # this is how much mu can vary\n",
    "    \n",
    "    #This is imposing a prior on the amplitude of the emission line\n",
    "    #min and max amplitude of the emission line\n",
    "    min_A = 0\n",
    "    max_A = Amp_max * 2\n",
    "    \n",
    "    #We also have a prior on the sigma or width of the emission line as it cannot be too small or too big so \n",
    "    #this prior takes this into account for this NIRSpec data. If you are working with different data\n",
    "    #all the priors will need to be updated\n",
    "    sigma_window_left = .0001 #had to change these for the input spectra these are left bounds for sigma\n",
    "    sigma_window_right = .005 #had to change these for the input spectra these are right bounds for sigma\n",
    "\n",
    "    #This is a prior on the continuum of the spectra\n",
    "    min_cont = 0\n",
    "        \n",
    "    if (min_A < A < max_A) & (left_mu <= mu <= right_mu) & (sigma_window_left <= sigma < sigma_window_right) & (b > min_cont):\n",
    "        return 0.0\n",
    "    else:\n",
    "        return -np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d992e315",
   "metadata": {},
   "source": [
    "# 5. Merging the Likelihood and Priors Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e178eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_probability(theta, x, y, yerr, first_wave, Amp_max):\n",
    "    \n",
    "    lp = log_prior(theta, first_wave, Amp_max)\n",
    "    if not np.isfinite(lp):\n",
    "        #print('Probability is infinite')\n",
    "        return -np.inf\n",
    "    prob = lp + log_likelihood(theta, x, y, yerr)\n",
    "    #print(f'Prob:{prob:.3E}')\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10145b6",
   "metadata": {},
   "source": [
    "# Input Values\n",
    "Now that we have the infrastructure of emcee set we just have one more thing to do. Emcee does require an intial guess to start exploring the parameter space. While you can give it any starting point and, in theory, it should be able to converge at some point this may not be the most optimal strategy. We can try to provide emcee with some good initial guesses and then have emcee be more efficient when it explores the parameter space. So what we are going to do in the next cell is create a function that can take in a region around the emission line and provide some good initial guesses for emcee to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbddde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_fits(wave, spectrum, err_spec, window, line_center, diagnose = False):\n",
    "    \n",
    "    '''\n",
    "    This function does an initial fit on the data using curve fit which we then pass in those parameters into emcee\n",
    "    to do the full MCMC fit later\n",
    "    \n",
    "    Inputs\n",
    "    -------------\n",
    "    wave: Wavelength Array\n",
    "    spectrum: Full spectrum array\n",
    "    err_spec: the Error spectra\n",
    "    window: the window to look around an emission line in units of the wavelength array\n",
    "    line_center: The line center of the emission line\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    result: The output of the intial curve fit which would be an array with output in order of the parameters\n",
    "            in the model np.array([A, mu, sigma, b])\n",
    "    \n",
    "    '''\n",
    "\n",
    "    #the range where the optimization can look between \n",
    "    min_window = line_center - window\n",
    "    max_window = line_center + window\n",
    "    \n",
    "    #getting emission line near the line center\n",
    "    #line_center +/- window\n",
    "    indx = np.where((min_window < wave) & ((wave < max_window)))[0]\n",
    "\n",
    "    spec_window = spectrum[indx]\n",
    "    wave_window = wave[indx]\n",
    "    err_spec_window = err_spec[indx]\n",
    "    \n",
    "    #initial guesses for the optimization\n",
    "    guess_A = np.amax(spectrum[indx])\n",
    "    guess_mu = line_center\n",
    "    \n",
    "    #We interpolate the spectrum near the emission line, we do this to get an estimate on sigma by computing \n",
    "    #the full width at half-maximum\n",
    "    spec_interp = Akima1DInterpolator(wave_window, spec_window)\n",
    "    \n",
    "    #making a wavelength array near the emission line\n",
    "    x = np.linspace(wave_window[0], wave_window[-1], 10000)\n",
    "    \n",
    "    #applying the wavelength array to the interpolated function\n",
    "    spec = spec_interp(x)\n",
    "    \n",
    "    #getting the value at half maximum\n",
    "    half_max = np.amax(spec)/2\n",
    "    \n",
    "    #finding index where the spectrum is higher than the half-maximum value\n",
    "    #the first and last indexes are the wavelength where the sigma can be computed\n",
    "    idx = np.where(spec > half_max)[0]\n",
    "    \n",
    "    #getting the left and right most wavelengths\n",
    "    wave_left, wave_right = x[idx[0]], x[idx[-1]]\n",
    "    \n",
    "    #taking the difference between the right and left wavelength and divide it by 2 to get a guess for the sigma\n",
    "    guess_sigma = (wave_right - wave_left)/2\n",
    "\n",
    "    cont_wave = np.append(wave_window[:3], wave_window[-3:])\n",
    "    cont_spec = np.append(spec_window[:3], spec_window[-3:])\n",
    "\n",
    "    params = np.polyfit(cont_wave, cont_spec, 1 )\n",
    "\n",
    "    guess_m = params[0]\n",
    "\n",
    "    guess_b = np.median(spec_window)\n",
    "    \n",
    "    if diagnose == True:\n",
    "        \n",
    "        print('Minimization Guesses')\n",
    "        print(f\"A: {guess_A}\")\n",
    "        print(f\"mu: {guess_mu}\")\n",
    "        print(f\"sigma: {guess_sigma}\")\n",
    "        print(f\"m: {guess_m}\")\n",
    "        print(f\"b: {guess_b}\")\n",
    "        print() \n",
    "\n",
    "    #making initial guesses\n",
    "    x0 = [guess_A, guess_mu, guess_sigma, guess_m, guess_b]\n",
    "\n",
    "    #making lower and upper bounds to use into curve_fit\n",
    "    low_bounds = [0, min_window, 0, -1, guess_b/2]\n",
    "    high_bounds = [2*guess_A, max_window, .005, 1, guess_b*2]\n",
    "    #the above code is similar to the log_prior function but for curve_fit where we tell curve_fit to only look between\n",
    "    #these bounds\n",
    "\n",
    "    # Optimization of the initial gaussian fit\n",
    "    result,_ = curve_fit(line_model, #the function we are fitting\n",
    "                         wave_window, #the x array\n",
    "                         spec_window, #the y array\n",
    "                         p0 = x0,     #the initial guesses\n",
    "                        bounds = [low_bounds, high_bounds])                 \n",
    "    \n",
    "    \n",
    "    ########\n",
    "    # Diagnostic Plotting: making sure we are getting the emission line\n",
    "    ########\n",
    "    if diagnose == True:\n",
    "        \n",
    "        print('Minimization Results')\n",
    "        print(f\"A: {result[0]}\")\n",
    "        print(f\"mu: {result[1]}\")\n",
    "        print(f\"sigma: {result[2]}\")\n",
    "        print(f\"m: {result[3]}\")\n",
    "        print(f\"b: {result[4]}\")\n",
    "        print()\n",
    "        \n",
    "        xarr = np.linspace(wave_window[0], wave_window[-1], 100)\n",
    "        plt.figure()\n",
    "        plt.plot(wave_window, spec_window, color = 'blue', label = 'Data')\n",
    "        plt.scatter(wave_window, spec_window, color = 'blue')\n",
    "        plt.plot(xarr, line_model(xarr, *result), color = 'black', label = 'Model')\n",
    "        plt.axhline(0, linestyle = '--')\n",
    "        plt.ylabel('Flux')\n",
    "        plt.xlabel(r'Wavelength $\\mu$m')\n",
    "        plt.title('Initial curve_fit Fitting')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff95878",
   "metadata": {},
   "source": [
    "# 5 Using Emcee\n",
    "\n",
    "Emcee is a fully Bayesian way to fit a model to a data. The steps involved for using emcee is to define three functions:\n",
    "\n",
    "1. log_prior: This is a function that holds any prior conditions on the parameters you are trying to fit\n",
    "2. log_likelihood: This is a function that tells emcee how likely the current parameters are at recreating the data\n",
    "3. log_probability: This is the summation of log_prior and log_likelihood and gives the probability of that\n",
    "                    This is something that we want to maximize as we want the sets of parameters to give us the \n",
    "                    most probability\n",
    "\n",
    "\n",
    "Which we covered in the above cells. Now that we have all the pieces in place we can now go ahead and run emcee on the spectrum.\n",
    "\n",
    "The way emcee works is that we need to generate what are called \"walkers\". These walkers explore the parameter space and they use the log_likelihood and log_prior to test if the parameter they are exploring is a good one or a bad one. They use the output of log_prior and log_likelihood to walk towards the parameters where the model is the best fit to the data. Then due to MCMC at some point the walkers should converge and once they have converged you can use those parameters after the walkers have converged. Let us see that in action in the function below.\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84ca1ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitting_line(wave, flux, flux_err, line_center, window_wavelength, run = 3000,\n",
    "                 diagnose = False):\n",
    "    \n",
    "    '''\n",
    "    The code that fits the line using the emcee approach\n",
    "    \n",
    "    Inputs\n",
    "    -----------\n",
    "    wave: Wavelength array\n",
    "    flux: Flux array\n",
    "    flux_err: Flux error array\n",
    "    line_center: the line center\n",
    "    window_wavelength: The window near emission line in units of wavelength\n",
    "    run: how many iterations to run emcee on default is 3000\n",
    "    diagnose: An optional argument to output diagnostic plots as the fitting is proceeding\n",
    "              Outputs plots from the initial fits, walker locations prior to using emcee and output emcee plots\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    emcee_wave: The wavelength array used in the emcee fitting \n",
    "    emcee_spec: the flux spectra array used in the emcee fitting\n",
    "    emcee_err: The error array used in the emcee fitting \n",
    "    emcee_df: the output emcee data frame with parameter values and flux estimates using Flux = A*sigma*sqrt(2 pi)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #calling the function that does the initial fitting\n",
    "    result = initial_fits(wave, flux, flux_err, window_wavelength, line_center, diagnose = diagnose)\n",
    "    \n",
    "    #getting the results from the initial fit to then pass into emcee\n",
    "    guess_A = result[0]\n",
    "    guess_mu = result[1]\n",
    "    guess_sigma = result[2]\n",
    "    guess_m = result[3]\n",
    "    guess_b = result[4]\n",
    "    \n",
    "    ############################################################################################################################\n",
    "    #The code below is generating walkers for every single parameter we are trying to fit\n",
    "    #The walkers are generated by taking a normal distribution centered on the best fit value from the curve_fit\n",
    "    #The scale of the normal distribution is tailored to the parameter we are fitting\n",
    "    #each of the jump parameters are then reshaped into a column vector and stacked horizontally to make the starting walkers\n",
    "    ############################################################################################################################\n",
    "\n",
    "    #making walkers so that we can use emcee to explore the parameter space\n",
    "    #centered on the best results from minimization\n",
    "    amp_jump = np.random.normal(loc = guess_A,            #centered on best A from curve_fit\n",
    "                                scale = guess_A/10,       #can wander 1/10 of the value of A\n",
    "                                size = 32).reshape(-1, 1) \n",
    "    \n",
    "    wavelength_jump = np.random.normal(loc = guess_mu,    #centered on best mu from curve_fit\n",
    "                                       scale = .005,      #can wander +/- 0.005 microns (again tailored to nirspec\n",
    "                                       size = 32).reshape(-1, 1)#data so if you are working with other spectra \n",
    "                                                                #you may need ot update this)\n",
    "    \n",
    "    sigma_jump = np.random.normal(loc = guess_sigma,       #centered on best sigma from curve_fit\n",
    "                                  scale = .002,            #can wander +/- 0.002 microns (tailored for nirspec data)\n",
    "                                  size = 32).reshape(-1, 1)\n",
    "\n",
    "    \n",
    "    powerm = np.log10(np.abs(guess_m))\n",
    "    \n",
    "    #\n",
    "    m_jump = np.random.normal(loc = guess_m,           #centered on best b from curve_fit\n",
    "                              scale = 1*10**powerm,    #making it wander 10^powerb (if b = .05, it can wander .01)\n",
    "                              size = 32).reshape(-1, 1)\n",
    "    \n",
    "    #getting the power of 10 that the linear fit is\n",
    "    powerb = np.log10(np.abs(guess_b))\n",
    "    \n",
    "    #\n",
    "    b_jump = np.random.normal(loc = guess_b,           #centered on best b from curve_fit\n",
    "                              scale = 1*10**powerb,    #making it wander 10^powerb (if b = .05, it can wander .01)\n",
    "                              size = 32).reshape(-1, 1)\n",
    "\n",
    "    \n",
    "    #################\n",
    "    # Diagnostic plotting to see if the parameters were jumping to large values\n",
    "    # The should be concentrated near their best fit results values\n",
    "    #################\n",
    "    if diagnose == True:\n",
    "        print('Checking the Walker Jumps')\n",
    "        fig, ax = plt.subplots(nrows = 2, ncols = 2, constrained_layout = True)\n",
    "        \n",
    "        ax[0, 0].hist(amp_jump)\n",
    "        ax[0, 0].set_xlabel('Amplitude')\n",
    "        \n",
    "        ax[0, 1].hist(wavelength_jump)\n",
    "        ax[0, 1].set_xlabel(r'$\\mu$')\n",
    "        \n",
    "        ax[1, 0].hist(sigma_jump)\n",
    "        ax[1, 0].set_xlabel(r'$\\sigma$')\n",
    "        \n",
    "        ax[1, 1].hist(b_jump)\n",
    "        ax[1, 1].set_xlabel('b')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "    #stacking along the columns and generating the starting walkers\n",
    "    starting_walkers = np.hstack((amp_jump,\n",
    "                                  wavelength_jump, \n",
    "                                  sigma_jump, \n",
    "                                  m_jump,\n",
    "                                  b_jump))\n",
    "\n",
    "    #initializing window for emcee around the best result mu\n",
    "    emcee_window = window_wavelength \n",
    "    \n",
    "    #getting indexes near the emission line based off of the emcee_window\n",
    "    #looking at line_center +/- emcee_window\n",
    "    emcee_indx = np.where((wave >= (line_center - emcee_window)) & \n",
    "                          (wave <= (line_center + emcee_window)))[0] \n",
    "\n",
    "    #emcee subsections\n",
    "    emcee_spec = flux[emcee_indx]\n",
    "    emcee_wave = wave[emcee_indx]\n",
    "    emcee_err = flux_err[emcee_indx]\n",
    "\n",
    "\n",
    "    #initializing walker positions\n",
    "    pos = starting_walkers\n",
    "    nwalkers, ndim = pos.shape\n",
    "\n",
    "    #initializing sampler\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, #giving emcee the walker positions\n",
    "                                    ndim,     #giving it the dimension of the model(same as number of model parameters)\n",
    "                                    log_probability, #giving it the log_probability function\n",
    "                                    args=(emcee_wave, emcee_spec, emcee_err, guess_mu, guess_A), #arguments to pass into log_probability\n",
    "                                    )\n",
    "\n",
    "    #running emcee for 1000 iterations\n",
    "    state = sampler.run_mcmc(pos, 1000)\n",
    "    #We then reset the sample as the first 1000 iterations are the burn-in \n",
    "    sampler.reset()\n",
    "    #we then run emcee for the desired number of iterations\n",
    "    sampler.run_mcmc(state, run, progress=False)\n",
    "\n",
    "    #getting values back\n",
    "    flat_samples = sampler.get_chain(flat=True)\n",
    "    LnL_chain = sampler.flatlnprobability \n",
    "    \n",
    "    emcee_df = pd.DataFrame()\n",
    "    emcee_df['A'] = flat_samples[:, 0]\n",
    "    emcee_df['mu'] = flat_samples[:, 1]\n",
    "    emcee_df['sigma'] = flat_samples[:, 2]\n",
    "    emcee_df['b'] = flat_samples[:, 3]\n",
    "    emcee_df['LnL'] = LnL_chain[:]\n",
    "    \n",
    "    #removing values where the log_likelihood was infinite as these are bad fits\n",
    "    emcee_df = emcee_df[np.isfinite(emcee_df.LnL.values)]\n",
    "    \n",
    "    #getting the flux from the parameter values\n",
    "    #Note this assumes that the spectra is in erg/s/cm^2/Angstrom and wavelengths are in Angstrom\n",
    "    fluxes_emcee = emcee_df['A'] * emcee_df['sigma'] * np.sqrt(2 * np.pi)\n",
    "    \n",
    "    emcee_df['Fluxes'] = fluxes_emcee\n",
    "    \n",
    "    if diagnose == True:\n",
    "        \n",
    "        print('Checking Prameter Posterior Distributions')\n",
    "        fig, ax = plt.subplots(nrows = 2, ncols = 2, constrained_layout = True)\n",
    "        \n",
    "        emcee_df.A.hist(ax = ax[0, 0])\n",
    "        emcee_df.mu.hist(ax = ax[0, 1])\n",
    "        emcee_df.sigma.hist(ax = ax[1, 0])\n",
    "        emcee_df.b.hist(ax = ax[1, 1])\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    if diagnose == True:\n",
    "        xarr = np.linspace(emcee_wave[0], emcee_wave[-1], 100)\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.title('Input Emcee Spectra and Emcee Fit')\n",
    "        plt.step(emcee_wave, emcee_spec, color = 'black', alpha = 0.5, label = 'Data')\n",
    "        plt.errorbar(emcee_wave, emcee_spec, yerr = emcee_err, fmt = 'o', color = 'black')\n",
    "        plt.plot(xarr, line_model(xarr, *emcee_df.quantile(q = 0.5).values[:-2]), label = 'Model')\n",
    "        plt.xlabel(r'Wavelength [$\\mu$m]')\n",
    "        plt.ylabel('Flux')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    ###########\n",
    "    #NOTE:\n",
    "    #need to also give the filename argument otherwise it will overwrite the default file\n",
    "    ###########\n",
    "    if save_df == True:\n",
    "        emcee_df.to_csv(filename, sep = ' ')\n",
    "        \n",
    "    else:\n",
    "        return emcee_wave, emcee_spec, emcee_err, emcee_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f23b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_window = .002\n",
    "\n",
    "#best fit lambda\n",
    "best_lamba_obs = 1.2874\n",
    "\n",
    "mask = (wavelength_cleaned > best_lamba_obs - line_window) & (wavelength_cleaned < best_lamba_obs + line_window)\n",
    "\n",
    "plt.figure()\n",
    "plt.step(wavelength_cleaned[mask], flux_cleaned[mask], where = 'mid', color = 'black', alpha = 0.5)\n",
    "plt.errorbar(wavelength_cleaned[mask], flux_cleaned[mask], yerr = flux_error_cleaned[mask], fmt = 'none', color = 'black', alpha = 0.5)\n",
    "plt.axvline(best_lamba_obs, color = 'red', linestyle = '--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82780e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#we perform the fitting on this line\n",
    "check_wave, check_flux, check_flux_err, check_df = fitting_line(wavelength_cleaned, \n",
    "                                                                flux_cleaned, flux_error_cleaned, \n",
    "                                                                best_lamba_obs, line_window, save_df = False,\n",
    "                                                                diagnose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "525ddaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cobaltblue = '#2e37fe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aeab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the histograms of the posterior distributions on our parameters\n",
    "check_df.hist(figsize = (12, 10), bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf55a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a wavelength array from the emcee wavelength array\n",
    "xarr = np.linspace(check_wave[0], check_wave[-1], 1000)\n",
    "\n",
    "#Getting the median parameters from the emcee fit\n",
    "median_param_vals = check_df.quantile(q = 0.5).values[:-2] #We use the [:-2] because the last two columns \n",
    "                                                           #are LnL and Flux\n",
    "                                                           #We do not need those for the line_model \n",
    "\n",
    "plt.figure(figsize = (10, 7))\n",
    "plt.step(check_wave, check_flux, where = 'mid', color = 'cadetblue', label = 'Data')\n",
    "plt.errorbar(check_wave, check_flux, yerr = check_flux_err, fmt = 'o', color = 'cadetblue')\n",
    "plt.plot(xarr, line_model(xarr, *median_param_vals), label = 'Model', color = cobaltblue)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c87bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
