{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Pandas\n",
    "\n",
    "- Sections\n",
    "\n",
    "- Introduction to Pandas\n",
    "\n",
    "- Creating and Loading Data\n",
    "\n",
    "- Exploring DataFrames\n",
    "\n",
    "- Data Cleaning\n",
    "\n",
    "- Visualization with Pandas\n",
    "\n",
    "- Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a DataFrame\n",
    "\n",
    "A DataFrame in pandas can be easily created from a Python dictionary, where the dictionary keys become column names and the values (lists, arrays, or Series) become the column data. This is a convenient way to organize structured data for analysis, since DataFrames support powerful tools for filtering, aggregation, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Garnet', 'Hannah', 'Kendall'],\n",
    "    'Age': [24, 27, 22, 32, 29, 45, 33, 23, np.nan],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Chicago', 'New York', 'Houston', np.nan],\n",
    "    'Salary': [70000, 80000, 50000, 120000, 75000, 150000, 234092, 43223, np.nan]\n",
    "}\n",
    "data_df = pd.DataFrame(data, index = [3434, 5434, 903, 24312, 443,4312, 35, 352, 431])\n",
    "print(\"\\n--- DataFrame ---\\n\\n\")\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful DataFrame Methods for Quick Data Analysis\n",
    "\n",
    "df.head() / df.tail() → preview the first or last rows of a table.\n",
    "\n",
    "df.info() → summary of column names, data types, and non-null counts.\n",
    "\n",
    "df.describe() → quick statistics (mean, std, min, max, quartiles) for numerical columns.\n",
    "\n",
    "df.shape → gives the number of rows and columns.\n",
    "\n",
    "df.columns → lists all column names.\n",
    "\n",
    "df.value_counts() → frequency counts of values in a column.\n",
    "\n",
    "df.isnull().sum() → check for missing values in each column.\n",
    "\n",
    "df.corr() → correlation matrix between numerical columns.\n",
    "\n",
    "df.groupby() → group data by a column for aggregation (e.g., averages, counts).\n",
    "\n",
    "df.sort_values() → sort rows by column values.\n",
    "\n",
    "df.duplicated().sum()  # Count duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.sort_values(by='Salary', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seeing duplicates in the DataFrame\n",
    "data_df.duplicated().sum()  # Count duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.drop_duplicates(inplace=True)  # Remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Data: \n",
    "\n",
    "In pandas, you can access data in a DataFrame using labels, positions, or boolean conditions. Two of the most common accessors are .loc and .iloc. \n",
    "\n",
    "## .loc()\n",
    "The .loc accessor is label-based, meaning you select rows and columns using their names or index labels. \n",
    "\n",
    "## .iloc()\n",
    "In contrast, .iloc is position-based, so you access rows and columns by their integer positions (like array indexing). \n",
    "\n",
    "For example, df.loc[5, \"flux\"] selects the row with index label 5 and the column \"flux\", while df.iloc[0, 2] selects the first row and third column regardless of labels. This distinction makes it easy to switch between human-readable label access and fast position-based indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"name\": [\"Sirius\", \"Vega\", \"Betelgeuse\"],\n",
    "    \"magnitude\": [14.56, 13.45, 12.345],\n",
    "    \"distance_ly\": [8.6, 25.0, 642.5]\n",
    "}\n",
    "df = pd.DataFrame(data, index=[\"star1\", \"star2\", \"star3\"])\n",
    "\n",
    "print(\"DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Using .loc (label-based)\n",
    "print(\"\\n.loc example (row label 'star2', column 'magnitude'):\")\n",
    "print(df.loc[\"star2\", \"magnitude\"])\n",
    "\n",
    "# Using .iloc (position-based)\n",
    "print(\"\\n.iloc example (row 1, column 1):\")\n",
    "print(df.iloc[1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading Data from a CSV file (example)\n",
    "\n",
    "To read in a CSV file you can use the pandas built in to_csv() function and it will automatically open up a CSV file in Python. Let us see this in action below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Boston.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# index_col\n",
    "\n",
    "The index_col argument within the read_csv is a python-based index of which column to use for the DataFrame Index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading in .txt files\n",
    "\n",
    "Pandas’ read_csv function can be used to open not only CSV files but also general text files containing structured tabular data. By specifying parameters like sep for the delimiter and index_col for which column should be used as the row index, you can easily read in whitespace- or tab-separated data. \n",
    "\n",
    "\n",
    "The command sep = r'\\s+' treats any amount of whitespace as a separator, and uses the first column as the index. The resulting DataFrame can then be used for exploration, filtering, and analysis just like any other pandas DataFrame.\n",
    "\n",
    "After loading a text file into a DataFrame with read_csv, it’s helpful to quickly inspect its structure before analysis. Useful commands include:\n",
    "\n",
    "df.head() → view the first few rows to check that data was read correctly.\n",
    "\n",
    "df.info() → see column names, data types, and non-null counts.\n",
    "\n",
    "df.describe() → get summary statistics for numerical columns.\n",
    "\n",
    "df.columns → list all column names to verify headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_txt = pd.read_csv(\"Masses_V2_Table.txt\", sep = r\"\\s+\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning (Handling Missing Values)\n",
    "\n",
    "In pandas, handling missing data is an important step before analysis. Missing values are usually represented as NaN, and pandas provides several methods to manage them. You can remove rows or columns containing missing data using dropna(), or fill in missing values with fillna()—for example, replacing them with a default value, the column mean, or a computed estimate. Cleaning missing data ensures that subsequent calculations, visualizations, and models are accurate and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to determine if there are NaNs in the columns and how many are there\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ways to handle NaNs\n",
    "#1. Drop the rows with NaNs\n",
    "df_no_missing = merge_df.dropna()\n",
    "df_no_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Fill NaNs with a specific value\n",
    "df_fill = merge_df.fillna(0)\n",
    "df_fill.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fill.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Fill NaNs with the mean of the column\n",
    "df_mean = merge_df.fillna(df.mean())\n",
    "df_mean.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Fill NaNs with the median of the column\n",
    "df_median = merge_df.fillna(df.median())\n",
    "df_median.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Fill NaNs with the mode of the column\n",
    "df_mode = merge_df.fillna(df.mode().iloc[0])\n",
    "df_mode.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Filtering\n",
    "\n",
    "Boolean masking in pandas is a powerful way to filter a DataFrame based on conditions. By creating a boolean array from a comparison or logical operation, you can select only the rows that satisfy certain criteria. This allows for quick subsetting of data without modifying the original DataFrame.\n",
    "\n",
    "Boolean masking is very useful in astronomy for filtering catalogs, selecting objects within a certain range of properties, or isolating outliers for further study.\n",
    "\n",
    "If you want, I can also write a blurb combining boolean masking with .loc for a more flexible way to filter both rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boolaen Masking to Filter the DataFrame\n",
    "df_filtered = df[df['dis'] < 2]\n",
    "print(\"\\nFiltered Data (dis < 1):\\n\")\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Visualization with Pandas\n",
    "\n",
    "Pandas DataFrames come with built-in plotting capabilities that make quick visualizations of your data simple. By calling the .plot() method on a DataFrame or Series, you can generate line plots, scatter plots, histograms, bar charts, and more. These plots are powered by Matplotlib under the hood, so you can customize them further if needed. Built-in plots are especially useful for exploratory data analysis, allowing you to quickly spot trends, distributions, or outliers in your astronomical datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='indus', y='crim', kind='scatter', title='Crime vs Industry')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='indus', y='crim', kind='scatter', title='Crime vs Industry')\n",
    "plt.xlabel('Industry')\n",
    "plt.ylabel('Crime')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visualization with Pandas Friendly Packages (seaborn) \n",
    "\n",
    "Pandas works seamlessly with data visualization libraries like Seaborn, which is built on top of Matplotlib and designed for statistical plotting. Seaborn can directly accept pandas DataFrames, making it easy to create complex plots such as scatter plots with regression lines, histograms, kernel density estimates, pair plots, and categorical plots. Its integration with pandas means you can filter, group, or aggregate your DataFrame first, then pass it straight to Seaborn for visualization—perfect for exploring relationships in astronomical datasets.\n",
    "\n",
    "Seaborn makes it much easier to produce informative, publication-quality plots directly from pandas DataFrames compared to using Matplotlib alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.read_csv('Features_with_Continuum.txt', sep = ' ', index_col = 0)\n",
    "predictors = pd.read_csv('Predictions_with_Continuum.txt', sep = ' ', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering the data\n",
    "good_fits_mask = features_df.chisq_phot < 50\n",
    "EW_r_mask = predictors.EW_r.values < 500\n",
    "total_mask = good_fits_mask & EW_r_mask\n",
    "\n",
    "good_fits_data = features_df[total_mask]\n",
    "y_pred = predictors[total_mask].EW_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize = (10, 10))\n",
    "\n",
    "ax = axes.flatten()\n",
    "\n",
    "cols= ['burst', 'dust:Av', 'stellar_mass', 'sfr']\n",
    "\n",
    "for column, a in zip(cols, ax):\n",
    "    sb.boxplot(good_fits_data[column], ax = a)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Data Analysis\n",
    "\n",
    "Part of our bread and butter is that we can take in a data set and learn things from it. This is the essence of data analysis where we use the data to uncover trends hidden within the data and we usually use plots and summary statistics to understand what the data is trying to tell us. We will cover some of the data analysis plots and techniques in the next few cells to familiarize yourself with what it means to analyze a data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize = (10, 10))\n",
    "\n",
    "ax = axes.flatten()\n",
    "\n",
    "cols= ['burst', 'dust:Av', 'stellar_mass', 'sfr']\n",
    "\n",
    "for column, a in zip(cols, ax):\n",
    "    a.hist(good_fits_data[column], bins = 30, color = 'purple')\n",
    "    a.set_xlabel(column)\n",
    "\n",
    "ax[0].set_ylabel('Counts')\n",
    "ax[-2].set_ylabel('Counts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize = (10, 10))\n",
    "\n",
    "ax = axes.flatten()\n",
    "\n",
    "cols= ['burst', 'dust:Av', 'stellar_mass', 'sfr']\n",
    "\n",
    "for column, a in zip(cols, ax):\n",
    "    a.scatter(good_fits_data[column], y_pred, color = 'purple', alpha = 0.5, s = 10)\n",
    "    a.set_xlabel(column)\n",
    "\n",
    "ax[0].set_ylabel('EW_r')\n",
    "ax[-2].set_ylabel('EW_r')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_fits_data[['burst', 'dust:Av', 'stellar_mass', 'sfr']].plot(kind = 'box', \n",
    "                                                                 subplots = True, \n",
    "                                                                 layout = (2, 2), \n",
    "                                                                 figsize = (10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corner Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "sb.pairplot(good_fits_data[['burst', 'dust:Av', 'stellar_mass', 'sfr']], corner = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr_matrix = good_fits_data[['burst', 'dust:Av', 'stellar_mass', 'sfr']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 5))\n",
    "sb.heatmap(corr_matrix, annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
